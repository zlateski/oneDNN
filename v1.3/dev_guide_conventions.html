<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.5"/>
<title>DNNL: Naming Conventions</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML,dnnl"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">Deep Neural Network Library (DNNL)
   &#160;<span id="projectnumber">1.3.0</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.5 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li><a href="examples.html"><span>Examples</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Namespaces</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&#160;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(9)"><span class="SelectionMark">&#160;</span>Groups</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(10)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Naming Conventions </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The DNNL documentation relies on a set of standard naming conventions for variables. This section describes these conventions.</p>
<h2>Variable (Tensor) Names</h2>
<p>Neural network models consist of operations of the following form: </p>
<p class="formulaDsp">
\[ \dst = f(\src, \weights), \]
</p>
<p> where \(\dst\) and \(\src\) are activation tensors, and \(\weights\) are learnable tensors.</p>
<p>The backward propagation consists then in computing the gradients with respect to the \(\src\) and \(\weights\) respectively: </p>
<p class="formulaDsp">
\[ \diffsrc = df_{\src}(\diffdst, \src, \weights, \dst), \]
</p>
<p> and </p>
<p class="formulaDsp">
\[ \diffweights = df_{\weights}(\diffdst, \src, \weights, \dst). \]
</p>
<p>While DNNL uses <em>src</em>, <em>dst</em>, and <em>weights</em> as generic names for the activations and learnable tensors, for a specific operation there might be commonly used and widely known specific names for these tensors. For instance, the <a class="el" href="dev_guide_convolution.html">convolution</a> operation has a learnable tensor called bias. For usability reasons, DNNL primitives use such names in initialization or other functions to simplify the coding.</p>
<p>To summarize, DNNL uses the following commonly used notations for tensors:</p>
<table class="doxtable">
<tr>
<th align="left">Name </th><th align="left">Meaning  </th></tr>
<tr>
<td align="left"><code>src</code> </td><td align="left">Source tensor </td></tr>
<tr>
<td align="left"><code>dst</code> </td><td align="left">Destination tensor </td></tr>
<tr>
<td align="left"><code>weights</code> </td><td align="left">Weights tensor </td></tr>
<tr>
<td align="left"><code>bias</code> </td><td align="left">Bias tensor (used in <a class="el" href="dev_guide_convolution.html">Convolution</a>, <a class="el" href="dev_guide_inner_product.html">Inner Product</a> and other primitives) </td></tr>
<tr>
<td align="left"><code>scale_shift</code> </td><td align="left">Scale and shift tensors (used in <a class="el" href="dev_guide_batch_normalization.html">Batch Normalization</a> and <a class="el" href="dev_guide_layer_normalization.html">Layer Normalization</a>) </td></tr>
<tr>
<td align="left"><code>workspace</code> </td><td align="left">Workspace tensor that carries additional information from the forward propagation to the backward propagation </td></tr>
<tr>
<td align="left"><code>scratchpad</code> </td><td align="left">Temporary tensor that is required to store the intermediate results </td></tr>
<tr>
<td align="left"><code>diff_src</code> </td><td align="left">Gradient tensor with respect to the source </td></tr>
<tr>
<td align="left"><code>diff_dst</code> </td><td align="left">Gradient tensor with respect to the destination </td></tr>
<tr>
<td align="left"><code>diff_weights</code> </td><td align="left">Gradient tensor with respect to the weights </td></tr>
<tr>
<td align="left"><code>diff_bias</code> </td><td align="left">Gradient tensor with respect to the bias </td></tr>
<tr>
<td align="left"><code>diff_scale_shift</code> </td><td align="left">Gradient tensor with respect to the scale and shift </td></tr>
<tr>
<td align="left"><code>*_layer</code> </td><td align="left">RNN layer data or weights tensors </td></tr>
<tr>
<td align="left"><code>*_iter</code> </td><td align="left">RNN recurrent data or weights tensors </td></tr>
</table>
<h2>Formulas and Verbose Output</h2>
<p>DNNL uses the following notations in the documentation formulas and verbose output. Here, lower-case letters are used to denote indices in a particular spatial dimension, the sizes of which are denoted by corresponding upper-case letters.</p>
<table class="doxtable">
<tr>
<th align="left">Name </th><th align="left">Semantics  </th></tr>
<tr>
<td align="left"><code>n</code> (or <code>mb</code>) </td><td align="left">batch </td></tr>
<tr>
<td align="left"><code>g</code> </td><td align="left">groups </td></tr>
<tr>
<td align="left"><code>oc</code>, <code>od</code>, <code>oh</code>, <code>ow</code> </td><td align="left">output channels, depth, height, and width </td></tr>
<tr>
<td align="left"><code>ic</code>, <code>id</code>, <code>ih</code>, <code>iw</code> </td><td align="left">input channels, depth, height, and width </td></tr>
<tr>
<td align="left"><code>kd</code>, <code>kh</code>, <code>kw</code> </td><td align="left">kernel (filter) depth, height, and width </td></tr>
<tr>
<td align="left"><code>sd</code>, <code>sh</code>, <code>sw</code> </td><td align="left">stride by depth, height, and width </td></tr>
<tr>
<td align="left"><code>dd</code>, <code>dh</code>, <code>dw</code> </td><td align="left">dilation by depth, height, and width </td></tr>
<tr>
<td align="left"><code>pd</code>, <code>ph</code>, <code>pw</code> </td><td align="left">padding by depth, height, and width </td></tr>
</table>
<h2>RNN-Specific Notation</h2>
<p>The following notations are used when describing RNN primitives.</p>
<table class="doxtable">
<tr>
<th align="left">Name </th><th align="left">Semantics  </th></tr>
<tr>
<td align="left">\(\cdot\) </td><td align="left">matrix multiply operator </td></tr>
<tr>
<td align="left">\( * \) </td><td align="left">element-wise multiplication operator </td></tr>
<tr>
<td align="left">W </td><td align="left">input weights </td></tr>
<tr>
<td align="left">U </td><td align="left">recurrent weights </td></tr>
<tr>
<td align="left">\(^T\) </td><td align="left">transposition </td></tr>
<tr>
<td align="left">B </td><td align="left">bias </td></tr>
<tr>
<td align="left">h </td><td align="left">hidden state </td></tr>
<tr>
<td align="left">a </td><td align="left">intermediate value </td></tr>
<tr>
<td align="left">x </td><td align="left">input </td></tr>
<tr>
<td align="left">\( _t {}_{}\) </td><td align="left">timestamp </td></tr>
<tr>
<td align="left">\( l \) </td><td align="left">layer index </td></tr>
<tr>
<td align="left">activation </td><td align="left">tanh, relu, logistic </td></tr>
<tr>
<td align="left">c </td><td align="left">cell state </td></tr>
<tr>
<td align="left">\(\tilde{c}\) </td><td align="left">candidate state </td></tr>
<tr>
<td align="left">i </td><td align="left">input gate </td></tr>
<tr>
<td align="left">f </td><td align="left">forget gate </td></tr>
<tr>
<td align="left">o </td><td align="left">output gate </td></tr>
<tr>
<td align="left">u </td><td align="left">update gate </td></tr>
<tr>
<td align="left">r </td><td align="left">reset gate </td></tr>
</table>
<h2>Memory Formats Tags</h2>
<p>When describing tensor memory formats, which is the DNNL term for the way that the data is laid out in memory, documentation uses letters of the English alphabet to describe an order of dimensions and their semantics.</p>
<p>The canonical sequence of letters is <em>a</em>, <em>b</em>, <em>c</em>, ..., <em>z</em>. In this notation, the <em>ab</em> tag denotes a two-dimensional tensor with <em>a</em> denoting the outermost dimension and <em>b</em> denoting the innermost dimension, where the latter is dense in memory. Further, the <em>ba</em> tag denotes a two-dimensional tensor but with last two dimensions transposed: instead of the naturally dense <em>b</em> dimension, now <em>a</em> is the dense dimension. If we suppose that the two-dimensional tensor is a matrix and the <em>a</em> and <em>b</em> dimensions represent the number of columns and rows, then <em>ab</em> would denote the row-major (C) format and <em>ba</em> would denote the column-major (Fortran) format.</p>
<dl class="todo"><dt><b><a class="el" href="todo.html#_todo000001">Todo:</a></b></dt><dd>Picture here</dd></dl>
<p>Upper-case letters are used to indicate that the data is laid out in blocks for a particular dimension. In such cases, the format name contains both upper- and lower-case letters for that dimension with a lower-case letter preceded by the block size. For example, the <em>Ab16a</em> tag denotes a format similar to row-major but with columns split into contiguous blocks of 16 elements each. Moreover, the implicit assumption is that if the number of columns is not divisible by 16, the last block in the in-memory representation will contain padding zeroes.</p>
<dl class="todo"><dt><b><a class="el" href="todo.html#_todo000002">Todo:</a></b></dt><dd>Picture here</dd></dl>
<p>Since there are many widely used names for specific deep learning domains like convolutional neural networks (CNNs), DNNL also supports memory format tags in which dimensions have specifically assigned meaning like 'image width', 'image height', etc. The following table summarizes notations used in such memory format tags.</p>
<table class="doxtable">
<tr>
<th>Letter </th><th>Dimension  </th></tr>
<tr>
<td>n </td><td>batch </td></tr>
<tr>
<td>g </td><td>groups </td></tr>
<tr>
<td>c </td><td>channels </td></tr>
<tr>
<td>o </td><td>output channels </td></tr>
<tr>
<td>i </td><td>input channels </td></tr>
<tr>
<td>h </td><td>height </td></tr>
<tr>
<td>w </td><td>width </td></tr>
<tr>
<td>d </td><td>depth </td></tr>
<tr>
<td>t </td><td>timestamp </td></tr>
<tr>
<td>l </td><td>layer </td></tr>
<tr>
<td>d </td><td>direction </td></tr>
<tr>
<td>g </td><td>gate </td></tr>
<tr>
<td>s </td><td>state </td></tr>
</table>
<p>The canonical sequence of dimensions for four-dimensional data tensors in CNNs is (batch, channels, spatial dimensions). Spatial dimensions are ordered for tensors with three spatial dimensions as (depth, height, width), for tensors with two spatial dimensions as (height, width), and as just (width) for tensors with only one spatial dimension.</p>
<p>In this notation, <em>nchw</em> is a memory format tag for a four-dimensional tensor, with the first dimension corresponding to batch, the second to channel, and the remaining two to spatial dimensions. Due to the canonical order of dimensions for CNNs, this tag is the same as <em>abcd</em>. As another example, <em>nhwc</em> is the same as <em>acdb</em>. </p>
</div></div><!-- contents -->
<div class="footer">
    <div class="footer-wrapper">
        <ul id="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>