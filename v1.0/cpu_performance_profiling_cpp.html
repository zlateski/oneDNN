<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.5"/>
<title>Intel(R) MKL-DNN: Performance Profiling Example</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)
   &#160;<span id="projectnumber">1.0.4</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.5 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li><a href="examples.html"><span>Examples</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&#160;</span>Groups</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(9)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Performance Profiling Example </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><blockquote class="doxtable">
<p>Example code: <a class="el" href="cpu_performance_profiling_8cpp-example.html">cpu_performance_profiling.cpp</a> </p>
</blockquote>
<p>This example uses <a class="el" href="dev_guide_verbose.html">MKLDNN_VERBOSE</a> trace output to tune Intel MKL-DNN code to align with the <a class="el" href="dev_guide_inference.html">best practices</a>.</p>
<p>It will assume knowledge of memory formats and their usage in Intel MKL-DNN. You can read more about this topic <a class="el" href="cpu_memory_format_propagation_cpp.html">here</a>.</p>
<p>The example has three different implementations of the mathematical operation:</p>
<ol type="1">
<li><em>Naive implementation</em> executes 2D convolution followed by ReLU on the data in <b>NCHW</b> format. This implementation does not align with Intel MKL-DNN best practices and results in suboptimal performance.</li>
<li><em>Blocked format implementation</em> executes the same operations sequence on the <b>blocked format</b> optimized for convolution performance. This implementation uses <code>format_tag=ANY</code> to create a convolution memory descriptor to determine the data format optimal for the convolution implementation. It then <b>propagates the blocked format</b> to the non-intensive ReLU. This implementation results in better overall performance than the naive implementation.</li>
<li><em>Fused implementation</em> executes convolution fused with ReLU on blocked data format. This implementation uses <code>format_tag=ANY</code> to create a convolution memory descriptor, and then adds ReLU as a <b>post-op</b> to the convolution primitive. This version implements all of the best practices for inference resulting in the best overall performance.</li>
</ol>
<h1><a class="anchor" id="cpu_performance_profiling_cpp_walkthrough"></a>
Walkthrough</h1>
<p>The program in <a class="el" href="cpu_performance_profiling_8cpp-example.html">cpu_performance_profiling.cpp</a> includes all three implementations introduced above. You can select the specific implementation using command line options.</p>
<p>After compilation, you can execute each implementation with: </p>
<div class="fragment"><div class="line">./program.exe implementation</div>
</div><!-- fragment --><p>Before you run the program, set your <code>MKLDNN_VERBOSE</code> environment variable to 1: </p>
<div class="fragment"><div class="line">export MKLDNN_VERBOSE=1</div>
</div><!-- fragment --><p>The program starts by creating Intel MKL-DNN memory objects in <b>NCHW</b> format. These are called <code>user_</code> because they are meant to represent the user's source data entering Intel MKL-DNN with the NCHW format.</p>
<div class="fragment"><div class="line">    <span class="comment">// set dimensions for synthetic data and weights</span></div>
<div class="line">    <span class="keyword">const</span> memory::dim BATCH = 1000;</div>
<div class="line">    <span class="keyword">const</span> memory::dim IC = 3, OC = 96;</div>
<div class="line">    <span class="keyword">const</span> memory::dim IH = 227, KH = 11, OH = 55;</div>
<div class="line">    <span class="keyword">const</span> memory::dim IW = 227, KW = 11, OW = 55;</div>
</div><!-- fragment --> <div class="fragment"><div class="line">    <span class="comment">// create MKL-DNN memory objects for user&#39;s tensors (in nchw and oihw formats)</span></div>
<div class="line">    <span class="comment">// @note here the library allocates memory</span></div>
<div class="line">    <span class="keyword">auto</span> user_src = memory({{BATCH, IC, IH, IW}, memory::data_type::f32,</div>
<div class="line">                    memory::format_tag::nchw}, cpu);</div>
<div class="line">    <span class="keyword">auto</span> user_wei = memory({{OC, IC, KH, KW}, memory::data_type::f32,</div>
<div class="line">                    memory::format_tag::oihw}, cpu);</div>
<div class="line">    <span class="keyword">auto</span> user_dst = memory({{BATCH, OC, OH, OW}, memory::data_type::f32,</div>
<div class="line">                    memory::format_tag::nchw}, cpu);</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>You can change the batch size to easily increase/decrease the workload.</dd></dl>
<p>The following descriptions of each implementation will reference each other, and are meant to be read in order.</p>
<h1><a class="anchor" id="cpu_performance_profiling_cpp_implementation1"></a>
Naive Implementation</h1>
<p>This implementation is launched with the following shell code: </p>
<div class="fragment"><div class="line">./program.exe naive</div>
</div><!-- fragment --><p> The program will call the implementation defined in the function <code>conv_relu_naive()</code>.</p>
<p>First it sets the dimensions and format for convolution memory descriptors (<code>_md</code>) to match <code>user_</code> values&ndash;one <code>md</code> each for source, destination, and weight data. Then it uses those <code>md</code> to create the convolution descriptor <code>conv_d</code>, which tells Intel MKL-DNN to use plain format (NCHW) for the convolution.</p>
<div class="fragment"><div class="line">    <span class="comment">// copy the dimensions and format from user&#39;s memory</span></div>
<div class="line">    <span class="keyword">auto</span> conv_src_md = memory::desc(user_src.get_desc());</div>
<div class="line">    <span class="keyword">auto</span> conv_wei_md = memory::desc(user_wei.get_desc());</div>
<div class="line">    <span class="keyword">auto</span> conv_dst_md = memory::desc(user_dst.get_desc());</div>
</div><!-- fragment --> <div class="fragment"><div class="line">    <span class="comment">// create a convolution descriptor</span></div>
<div class="line">    <span class="keyword">auto</span> conv_d = convolution_forward::desc(</div>
<div class="line">            prop_kind::forward_inference, algorithm::convolution_direct,</div>
<div class="line">            conv_src_md, conv_wei_md, conv_dst_md,</div>
<div class="line">            strides, padding, padding);</div>
</div><!-- fragment --><p> Next the program creates a convolution primitive descriptor <code>conv_pd</code> and convolution primitive <code>conv</code>. These structs will inherit NCHW format from <code>md</code> by way of the <code>conv_d</code>. Finally it creates the convolution primitive <code>conv</code> and adds it to the stream <code>s</code>, and then executes the <code>create_and_execute_relu(user_dst)</code> function.</p>
<div class="fragment"><div class="line">    <span class="comment">// create a convolution primitive descriptor</span></div>
<div class="line">    <span class="keyword">auto</span> conv_pd = convolution_forward::primitive_desc(conv_d, cpu);</div>
</div><!-- fragment --> <div class="fragment"><div class="line">    <span class="comment">// create convolution primitive</span></div>
<div class="line">    <span class="keyword">auto</span> conv = convolution_forward(conv_pd);</div>
</div><!-- fragment --> <div class="fragment"><div class="line">    <span class="comment">// execute convolution by adding it to the stream s</span></div>
<div class="line">    conv.execute(s, {</div>
<div class="line">            {MKLDNN_ARG_SRC, user_src},</div>
<div class="line">            {MKLDNN_ARG_WEIGHTS, user_wei},</div>
<div class="line">            {MKLDNN_ARG_DST, user_dst}});</div>
</div><!-- fragment --><div class="fragment"><div class="line">    <span class="comment">// execute relu (on convolution&#39;s destination format, whatever it is)</span></div>
<div class="line">    create_and_execute_relu(user_dst);</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>The function for creation and execution of ReLU primitive is defined elsewhere to keep this example clean. It is an non-intensive operation, so the <code>create_and_execute_relu()</code> function uses whatever the input data format is at the time it is called.</dd></dl>
<p>Using NCHW data format may result in suboptimal performance for compute intensives primitives, as shown in the following MKLDNN_VERBOSE output by the convolution and relu execution times of 235.9 and 100.3 milliseconds, respectively.</p>
<p><em>MKLDNN_VERBOSE output (see configuration notice*):</em> </p>
<div class="fragment"><div class="line">mkldnn_verbose,exec,convolution,gemm:jit,forward_inference,src_f32::</div>
<div class="line">        blocked:abcd:f0 wei_f32::blocked:abcd:f0 dst_f32::</div>
<div class="line">        blocked:abcd:f0,alg:convolution_direct,</div>
<div class="line">        mb1000_ic3oc96_ih227oh55kh11sh4dh0ph0_iw227ow55kw11sw4dw0pw0,235.86</div>
<div class="line">mkldnn_verbose,exec,eltwise,jit:avx512_common,forward_inference,</div>
<div class="line">        data_f32::blocked:abcd:f0,alg:<a class="code" href="group__cpp__api__enums.html#ggae45a07d6121fdd33e310782753178076aba09bebb742494255b90b43871c01c69">eltwise_relu</a>,1000x96x55x55,100.264</div>
</div><!-- fragment --><p> In <em>Blocked format implementation</em>, we will incorporate the best practice of letting Intel MKL-DNN determine the optimal format for convolution primitive.</p>
<h1><a class="anchor" id="cpu_performance_profiling_cpp_implementation2"></a>
Blocked format implementation</h1>
<p>This implementation is launched with the following shell code: </p>
<div class="fragment"><div class="line">./program.exe blocked</div>
</div><!-- fragment --><p> The program will call the implementation defined in the function <code>conv_relu_blocked()</code>.</p>
<p>First it creates the md as in <b>naive implementation</b>. Next it changes the <a class="el" href="structmkldnn_1_1memory.html#a123930e31c5460c2c5f052a59c2a4ced" title="Memory format tag specification. ">mkldnn::memory::format_tag</a> for each md to <code>ANY</code>. Then it uses those md to create the convolution descriptor conv_d, which tells Intel MKL-DNN to use whatever format it recommends for the convolution. Intel MKL-DNN will choose the CPU-friendly blocked format.</p>
<div class="fragment"><div class="line">    <span class="comment">// copy the dimensions and format from user&#39;s memory</span></div>
<div class="line">    <span class="keyword">auto</span> conv_src_md = memory::desc(user_src.get_desc());</div>
<div class="line">    <span class="keyword">auto</span> conv_wei_md = memory::desc(user_wei.get_desc());</div>
<div class="line">    <span class="keyword">auto</span> conv_dst_md = memory::desc(user_dst.get_desc());</div>
<div class="line"></div>
<div class="line">    <span class="comment">// reset format to &quot;any&quot; to allow convolution to pick the best implementation</span></div>
<div class="line">    conv_src_md.data.<a class="code" href="structmkldnn_1_1memory.html#a6466170ecf3556eb6ab15da9dcd51aa2">format_kind</a> = <a class="code" href="group__c__api__types__generic.html#gga1f6c390306d4c8a438c0efbff08c0539a25ba80bb864a69c9fae70b4503e76167">mkldnn_format_kind_any</a>;</div>
<div class="line">    conv_wei_md.data.format_kind = <a class="code" href="group__c__api__types__generic.html#gga1f6c390306d4c8a438c0efbff08c0539a25ba80bb864a69c9fae70b4503e76167">mkldnn_format_kind_any</a>;</div>
<div class="line">    conv_dst_md.data.format_kind = <a class="code" href="group__c__api__types__generic.html#gga1f6c390306d4c8a438c0efbff08c0539a25ba80bb864a69c9fae70b4503e76167">mkldnn_format_kind_any</a>;</div>
</div><!-- fragment --> <div class="fragment"><div class="line">    <span class="comment">// create a convolution descriptor</span></div>
<div class="line">    <span class="keyword">auto</span> conv_d = convolution_forward::desc(</div>
<div class="line">            prop_kind::forward_inference, algorithm::convolution_direct,</div>
<div class="line">            conv_src_md, conv_wei_md, conv_dst_md,</div>
<div class="line">            strides, padding, padding);</div>
</div><!-- fragment --><p> Next the program creates a convolution primitive descriptor conv_pd and convolution primitive conv as in naive implementation. However, in this implementation the structs will inherit blocked format from md by way of the conv_d.</p>
<div class="fragment"><div class="line">    <span class="comment">// create a convolution primitive descriptor and primitive</span></div>
<div class="line">    <span class="keyword">auto</span> conv_pd = convolution_forward::primitive_desc(conv_d, cpu);</div>
</div><!-- fragment --><p> Since the resulting convolution primitive will expect blocked source data, conditional reorders are inserted to convert input data to blocked format if required. The input data user_src is NCHW, so this conditional will be triggered:</p>
<dl class="section note"><dt>Note</dt><dd>The reoders are applied using Intel MKL-DNN <code>reorder</code> primitive.</dd></dl>
<div class="fragment"><div class="line">    <span class="comment">// prepare convolution source</span></div>
<div class="line">    memory conv_src = user_src;</div>
<div class="line">    <span class="keywordflow">if</span> (conv_pd.src_desc() != user_src.get_desc()) {</div>
<div class="line">        conv_src = memory(conv_pd.src_desc(), cpu);</div>
<div class="line">        <span class="keyword">auto</span> r_pd = reorder::primitive_desc(user_src, conv_src);</div>
<div class="line">        reorder(r_pd).execute(s, user_src, conv_src);</div>
<div class="line">    }</div>
<div class="line"></div>
<div class="line">    <span class="comment">// prepare convolution weights</span></div>
<div class="line">    memory conv_wei = user_wei;</div>
<div class="line">    <span class="keywordflow">if</span> (conv_pd.weights_desc() != user_wei.get_desc()) {</div>
<div class="line">        conv_wei = memory(conv_pd.weights_desc(), cpu);</div>
<div class="line">        <span class="keyword">auto</span> r_pd = reorder::primitive_desc(user_wei, conv_wei);</div>
<div class="line">        reorder(r_pd).execute(s, user_wei, conv_wei);</div>
<div class="line">    }</div>
<div class="line"></div>
<div class="line">    <span class="comment">// prepare convolution destination</span></div>
<div class="line">    memory conv_dst = user_dst;</div>
<div class="line">    <span class="keywordflow">if</span> (conv_pd.dst_desc() != user_dst.get_desc())</div>
<div class="line">        conv_dst = memory(conv_pd.dst_desc(), cpu);</div>
</div><!-- fragment --><p> Finally it creates the convolution primitive <code>conv</code> and adds it to the stream <code>s</code> with the reordered data (<code>conv_src</code>, <code>conv_wei</code>, <code>conv_dst1</code>) as inputs and then executes the <code>create_and_execute_relu(conv_dst)</code> function.</p>
<div class="fragment"><div class="line">    <span class="comment">// create convolution primitive</span></div>
<div class="line">    <span class="keyword">auto</span> conv = convolution_forward(conv_pd);</div>
</div><!-- fragment --> <div class="fragment"><div class="line">    <span class="comment">// execute convolution by adding it to the stream s</span></div>
<div class="line">    conv.execute(s, {</div>
<div class="line">            {MKLDNN_ARG_SRC, conv_src},</div>
<div class="line">            {MKLDNN_ARG_WEIGHTS, conv_wei},</div>
<div class="line">            {MKLDNN_ARG_DST, conv_dst}});</div>
</div><!-- fragment --> <div class="fragment"><div class="line">    <span class="comment">// execute relu (on convolution&#39;s destination format, whatever it is)</span></div>
<div class="line">    create_and_execute_relu(conv_dst);</div>
</div><!-- fragment --><p> Blocked memory format is recommended for Intel MKL-DNN primitive execution and provides better performance, as shown in the MKLDNN_VERBOSE output by the convolution and relu execution times of 119.6 and 34.4 milliseconds (down from 235.9 and 100.3 in <em>naive implementation</em>), respectively. In this implementation, there is an additional reorder operation that executes before and after the the conv + relu. This small cost is worth the gain from executing in blocked format. If fact, it becomes negligible when chaining together multiple Intel Mkl-DNN operations in succession. In these situations, you can do one reorder at the beginning and one at the end of the chain, and only pay the reorder penalty at those points in the execution.</p>
<p><em>MKLDNN_VERBOSE output (see configuration notice*):</em> </p>
<div class="fragment"><div class="line">mkldnn_verbose,exec,reorder,jit:uni,undef,src_f32::blocked:abcd:f0</div>
<div class="line">        dst_f32::blocked:Acdb16a:f0,num:1,96x3x11x11,3.71387</div>
<div class="line">mkldnn_verbose,exec,convolution,jit:avx512_common,forward_inference,</div>
<div class="line">        src_f32::blocked:abcd:f0 wei_f32::blocked:Acdb16a:f0</div>
<div class="line">        dst_f32::blocked:aBcd16b:f0,alg:convolution_direct,</div>
<div class="line">        mb1000_ic3oc96_ih227oh55kh11sh4dh0ph0_iw227ow55kw11sw4dw0pw0,119.649</div>
<div class="line">mkldnn_verbose,exec,eltwise,jit:avx512_common,forward_inference,</div>
<div class="line">        data_f32::blocked:aBcd16b:f0,alg:<a class="code" href="group__cpp__api__enums.html#ggae45a07d6121fdd33e310782753178076aba09bebb742494255b90b43871c01c69">eltwise_relu</a>,1000x96x55x55,34.417</div>
<div class="line">mkldnn_verbose,exec,reorder,jit:uni,undef,src_f32::blocked:aBcd16b:f0</div>
<div class="line">        dst_f32::blocked:abcd:f0,num:1,1000x96x55x55,97.3352</div>
</div><!-- fragment --><p> This inference implementation is closer to best practices than <em>naive implementation</em> because it uses Intel MKL-DNN recommended memory format. <em>fused implementation</em> will futher optimize the performance by using a fused version of the conv + ReLU primitive emplying the Intel MKL-DNN <a class="el" href="dev_guide_attributes_post_ops.html">post-ops attribute</a></p>
<h1><a class="anchor" id="cpu_performance_profiling_cpp_implementation3"></a>
Fused Implementation</h1>
<p>This implementation is launched with the following shell code: </p>
<div class="fragment"><div class="line">./program.exe fused</div>
</div><!-- fragment --><p> The program will call the implementation defined in the function <code>conv_relu_fused()</code>.</p>
<p>First the memory descriptors and convolution descriptor are created as in <em>naive implementation</em>.</p>
<p>Then in preparation for the convolution prim desctiptor, a ReLU post-op is built and added to the primitive attribute <code>attr</code>:</p>
<div class="fragment"><div class="line"><span class="comment">// function to create post-op attribute for fused relu</span></div>
<div class="line">primitive_attr create_attr_with_relu_post_op() {</div>
<div class="line">    <span class="comment">// create a post-op with relu</span></div>
<div class="line">    post_ops ops;</div>
<div class="line">    ops.append_eltwise(1.f, algorithm::eltwise_relu, 0.f, 0.f);</div>
<div class="line"></div>
<div class="line">    <span class="comment">// create an attribute and set the corresponding post op</span></div>
<div class="line">    primitive_attr attr;</div>
<div class="line">    attr.set_post_ops(ops);</div>
<div class="line"></div>
<div class="line">    <span class="keywordflow">return</span> attr;</div>
<div class="line">}</div>
</div><!-- fragment --><p> post-op by way of the attributes <code>attr</code>:</p>
<div class="fragment"><div class="line">    <span class="comment">// create an attribute for fused relu</span></div>
<div class="line">    <span class="keyword">auto</span> attr = create_attr_with_relu_post_op();</div>
<div class="line"></div>
<div class="line">    <span class="comment">// create a convolution primitive descriptor</span></div>
<div class="line">    <span class="keyword">auto</span> conv_pd = convolution_forward::primitive_desc(conv_d, attr, cpu);</div>
</div><!-- fragment --><p> Then conditional reorders are applied as in <em>blocked format implementation</em> to convert <code>user_</code> format NCHW to blocked. Finally, it creates the convolution primitive <code>conv</code> and adds it to the stream <code>s</code> with the reordered data (<code>conv_src</code>, <code>conv_wei</code>, <code>conv_dst1</code>).</p>
<dl class="section note"><dt>Note</dt><dd>There is no separate addition to the stream for the ReLU operation because it has been added as a post-op to the <code>conv</code> primitive.</dd></dl>
<div class="fragment"><div class="line">    <span class="comment">// create convolution primitive</span></div>
<div class="line">    <span class="keyword">auto</span> conv = convolution_forward(conv_pd);</div>
</div><!-- fragment --> <div class="fragment"><div class="line">    <span class="comment">// execute convolution by adding it to the stream s</span></div>
<div class="line">    conv.execute(s, {</div>
<div class="line">            {MKLDNN_ARG_SRC, conv_src},</div>
<div class="line">            {MKLDNN_ARG_WEIGHTS, conv_wei},</div>
<div class="line">            {MKLDNN_ARG_DST, conv_dst}});</div>
</div><!-- fragment --><p> This implementation complies with best practices for f32 inference by using the Intel MKL-DNN recommended blocked format for convolution and adding ReLU as a post-op to execute a fused version of conv + ReLU. The consequence to following best practices can be seen in the execution time of the fused primitive of 103.9 milliseconds.</p>
<p><em>MKLDNN_VERBOSE output (see configuration notice*):</em> </p>
<div class="fragment"><div class="line">mkldnn_verbose,exec,convolution,jit:avx512_common,forward_inference,</div>
<div class="line">        src_f32::blocked:abcd:f0 wei_f32::blocked:Acdb16a:f0</div>
<div class="line">        dst_f32::blocked:aBcd16b:f0,alg:convolution_direct,</div>
<div class="line">        mb1000_ic3oc96_ih227oh55kh11sh4dh0ph0_iw227ow55kw11sw4dw0pw0,103.916</div>
</div><!-- fragment --><h1><a class="anchor" id="cpu_performance_profiling_cpp_roundup"></a>
Performance summary</h1>
<table class="doxtable">
<tr>
<th align="left">Implmentation </th><th align="right">Time, ms </th><th align="right">Cumulative speedup  </th></tr>
<tr>
<td align="left">Naive </td><td align="right">336.1 </td><td align="right">1.0 </td></tr>
<tr>
<td align="left">Blocked format </td><td align="right">154.0 </td><td align="right">2.2 </td></tr>
<tr>
<td align="left">Fused </td><td align="right">103.9 </td><td align="right">3.2 </td></tr>
</table>
<hr/>
<h1><a class="anchor" id="cpu_performance_profiling_cpp_config"></a>
Configuration Notice</h1>
<dl class="section note"><dt>Note</dt><dd>This example is meant to demonstrate Intel MKL-DNN best practices. </dd>
<dd>
It is not meant for benchmarking purposes. The platform is not fully </dd>
<dd>
optimized, so the primitive execution times are only relevant in </dd>
<dd>
relation to the other times in this example.</dd></dl>
<p>Runtime Settings:</p>
<ul>
<li>OMP_NUM_THREADS=14</li>
<li>KMP_AFFINITY=granularity=fine,compact,1,0</li>
</ul>
<p>Platform:</p>
<ul>
<li>CPU: Intel(R) Xeon(R) Platinum 8180M CPU @ 2.50GHz</li>
<li>Thread(s) per core: 2</li>
<li>Core(s) per socket: 28</li>
<li>Socket(s): 2</li>
<li>NUMA node(s): 2</li>
<li>RAM (DDR4): 1.45 TB </li>
</ul>
</div></div><!-- contents -->
<div class="footer">
    <div class="footer-wrapper">
        <ul id="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>