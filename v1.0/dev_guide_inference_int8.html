<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.5"/>
<title>Intel(R) MKL-DNN: Int8 Inference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)
   &#160;<span id="projectnumber">1.0.4</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.5 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li><a href="examples.html"><span>Examples</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&#160;</span>Groups</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(9)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Int8 Inference </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Introduction</h2>
<p>To push higher performance during inference computations, recent work has focused on computing at a lower precision (that is, shrinking the size of data for activations and weights) to achieve higher throughput. Eight-bit computations (referred to as int8) offer improved performance over higher-precision types because they enable packing more data into a single instruction, at the cost of reduced (but acceptable) accuracy.</p>
<h2>Int8 Workflow</h2>
<p>There are different ways to use lower precision to perform inference. Please go trough the <a class="el" href="dev_guide_attributes_quantization.html">Primitive Attributes: Quantization</a> page to get the initial understanding of what kind of quantization model Intel MKL-DNN supports.</p>
<h3>Quantization Process</h3>
<p>To operate with int8 data types from a higher-precision format (for example, 32-bit floating point), data must first be <em>quantized</em>. The quantization process converts a given input into a lower-precision format. The precision and accuracy factors are determined by the scaling factors.</p>
<h3>Scale</h3>
<p>The scale is usually obtained from sampling the dataset of previous executions in the original format (for example, the activations and weights from training in fp32) and is formulated as:</p>
<ul>
<li>\( R_{\{\alpha,w\}} = max(abs(T_{\{\alpha,w\}}))\)</li>
</ul>
<p>where \(T_{\{\alpha,w\}} {}_{}\) is a tensor corresponding to either the weights \(w\) or the activations \(\alpha\).</p>
<p>The purpose is to establish the range of values used in the computation, where selecting a proper scaling factor prevents over- or underflows during computation of the lower-precision results.</p>
<h3>Quantization Factor</h3>
<p>The next step is to calculate the <b>quantization factor</b> for converting the values into the corresponding int8 range. This is also known as the <b>scale</b> or <b>scaling factor</b> applied to the original high-precision values and is calculated as:</p>
<ul>
<li>\( Q_{\alpha} = \frac{255}{R_{\alpha}}\) is the quantization factor for activations with non-negative values.</li>
<li>\( Q_{w} = \frac{127}{R_{w}}\) is the quantization factor for weights.</li>
</ul>
<p>The low-precision values, known as the <b>quantized</b> activation, weights, and bias values, are calculated as:</p>
<ul>
<li>\(\alpha_{u8} = \lceil Q_{\alpha} \alpha_{f32} \rceil \in [0,255]\)</li>
<li>\(W_{s8} = \lceil Q_{w} W_{f32} \rceil \in [-127,127]\)</li>
<li>\(b_{s32} = \lceil Q_{\alpha} Q_{w} b_{f32} \rceil \in [-2^{31},2^{31}-1]\)</li>
</ul>
<p>where the function \( \lceil \rceil \) rounds to the selected rounding mode (typically determined by the MXCSR register; the default value is RoundNearestEven).</p>
<p>When the destination value (for example, from a convolution) is stored as a signed 32-bit integer, the result is bound to the same quantization <em>scaling</em> factors:</p>
<ul>
<li>\(X_{s32} = W_{s8} \times \alpha{u8} + b_{s32} \approx Q_{\alpha} Q_{\omega} X_{f32}\)</li>
<li>where \(X_{f32} = W_{f32} \times \alpha_{f32} + b_{f32}\)</li>
</ul>
<p>where the approximated value is due to the rounded values.</p>
<p>Inversely, the dequantized value is calculated as:</p>
<ul>
<li>\(X_{f32} \approx \frac{1}{Q_{\alpha} Q_{\omega}} X_{s32} \)</li>
</ul>
<h3>Quantization Example</h3>
<p>To show how the int8 parameters are obtained, suppose we first start off with a set of arbitrary high-precision input and output values. These values come from sampling a previously executed training run and are in their original 32-bit floating point format as:</p>
<ul>
<li>activations: \( T_{\alpha} = [15, 14, 15 ... 8, 11 ]\) where \( max(abs(T_{\alpha})) = 15\)</li>
<li>weights: \( T_{\omega} = [-5.1 , 6.8, ... -1.2, 9.8 ]\) where \( max(abs(T_{\omega})) = 9.8\)</li>
<li>bias: \( T_{\alpha} = [ 2.4, -5.2 ... -8 ]\) where \( max(abs(T_{\alpha})) = 8\)</li>
</ul>
<p>The scaling factors are:</p>
<ul>
<li>\( Q_{\alpha} = \frac{255}{R_{\alpha}} = \frac{255}{15} = 17 \)</li>
<li>\( Q_{w} = \frac{127}{R_{w}} = \frac{127}{9.8} = 12.96\)</li>
</ul>
<p>Finally, the quantized input values for the 8-bit operation are calculated as:</p>
<ul>
<li>\(\alpha_{u8} = \lceil Q_{\alpha} \alpha_{f32} \rceil\) \( = \lceil 17 \times [15, 14, ... 11 ] \rceil = [255, 238, ... 187] \)</li>
<li>\(W_{s8} = \lceil Q_{w} W_{f32} \rceil = \lceil 12.96 \times [-5.1 , 6.8, ... -1.2, 9.8 ] \rceil = [-66, 88, ... -15, 127] \)</li>
<li>\(b_{s32} = \lceil Q_{\alpha} Q_{w} b_{f32} \rceil = \lceil 17 \times 12.96 \times [ 2.4, -5.2 ... -8 ] \rceil = [528, -1145, ... -1762] \)</li>
</ul>
<p>These arrays are the new inputs for the int8 net.</p>
<h2>Intel MKL-DNN Support for Low-Precision int8 Primitives</h2>
<p>Intel MKL-DNN supports low-precision computations for inference through the int8 primitives. int8 primitives are ordinary Intel MKL-DNN primitives that have their input and output parameters configured to 8-bit types. int8 primitives are optimized for high performance on the compatible hardware (see <a class="el" href="dev_guide_data_types.html">Data Types</a>).</p>
<h3>Intel MKL-DNN Attributes</h3>
<p>Intel MKL-DNN primitive behaviour may be extended for additional functionalities involving output data transformation. These additional features are configured via <b>primitive attributes</b>. The primitive attributes definition is an opaque structure for passing extra parameters to a primitive descriptor. These parameters include a scaling factor and fused post-ops. All operation primitives support the attributes structure; however, some configurations are not implemented and result in <em>failed primitive creation</em>.</p>
<p>The <b>scaling factor</b>, as previously described, is known prior to the inference operation where the values are calculated from a set of formulas. In Intel MKL-DNN, the scaling factor is applied to the output of a primitive. Moreover, to perform input transformations (for example, source, bias, and weights), Intel MKL-DNN performs quantizing and dequantizing of data for int8 through the <b>Reorder Primitive</b>.</p>
<p>Intel MKL-DNN has two formats for defining the output scaling factor. Depending on the configuration set by the scaling mask, either the output is scaled uniformly across all the dimensions (<em>mask = 0</em>) or a set of scaling values is applied to specific dimensions, as explained below:</p>
<ul>
<li>A <em>single floating point value</em> shared across the tensor <div class="image">
<img src="img_singlescalar.png" alt="img_singlescalar.png"/>
<div class="caption">
Single-value scaling format</div></div>
</li>
<li>An array of floating point values each corresponding to a specific output channel <div class="image">
<img src="img_multiscalar.png" alt="img_multiscalar.png"/>
<div class="caption">
Multi-value scaling format</div></div>
 The <b>mask</b> parameter determines the dimension to which the scales array is applied, where the i<sup>th</sup>-bit(s) of mask selects the dimension or dimensions d<sub>i</sub> (where <em>d</em> is an <em>n</em>-dimensional output tensor with logical dimensions as [<em>d0, d1, ..., dn-1</em>]). For example:</li>
<li>The single-scale format always has mask = 0.</li>
<li>For a 5-dimensional tensor T[g0, o1,i2,h3,w4] where the numbering indicates the bit-index:<ul>
<li>A mask = 2 = 2<sup>1</sup> selects the output channel for scaling.</li>
<li>A mask = 3 = 2<sup>0</sup> | 2<sup>1</sup> selects the group and output channels.</li>
</ul>
</li>
</ul>
<p>Mask is always applied to the logical dimension; this is independent of the dimension format that the primitive might select. The dimensions in Intel MKL-DNN are defined as follows:</p>
<ul>
<li>2D dimensional data the order of dimensions is always: (n, c)</li>
<li>4D dimensional data the order is always: (n, c, h, w)</li>
<li>5D dimensional weights the order is always: (g, oc, ic, kh, kw)</li>
</ul>
<p>Fused <b>post-ops</b> allow chaining operations during the primitive computation. Note that the resulting output value from post-ops is always affected by the scaling factor. The supported operations are:</p>
<ul>
<li>Accumulation where the primitive sums the resulting values from previously computed activations as:<ul>
<li>\(dst[ ] \leftarrow scale * dst[] + op(...)\), instead of</li>
<li>\(dst[ ] \leftarrow op(...)\)</li>
</ul>
</li>
<li>Element-wise (eltwise) operation with kind, alpha and beta parameters as:<ul>
<li>\(dst[ ] \leftarrow scale * eltwise\_op ( op(...) )\), instead of</li>
<li>\(dst[ ] \leftarrow op(...)\)</li>
</ul>
</li>
</ul>
<p>The list of supported eltwise operations for int8 is currently limited to ReLU. For instance, post-ops may only configure a convolution with accumulation followed by eltwise (relu).</p>
<h2>Example</h2>
<p><a class="el" href="cpu_cnn_inference_int8_cpp.html">CNN int8 inference example</a> example walks through the steps of int8 inference. </p>
</div></div><!-- contents -->
<div class="footer">
    <div class="footer-wrapper">
        <ul id="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>